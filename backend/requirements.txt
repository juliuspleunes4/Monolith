fastapi==0.115.0
uvicorn[standard]==0.32.0
pydantic==2.9.0
pydantic-settings==2.6.0
python-multipart==0.0.12
aiosqlite==0.20.0
httpx==0.27.2
# TODO: For local inference without external services, install llama-cpp-python
# This requires Visual Studio C++ Build Tools on Windows
# For now, using a placeholder that will work without compilation
