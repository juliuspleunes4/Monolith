<div align="center">

# üóø Monolith

**A fully local, GPU-accelerated LLM chat application with one-command Docker deployment**

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE.md)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg?logo=docker&logoColor=white)](https://www.docker.com/)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115.0-009688.svg)](https://fastapi.tiangolo.com)
[![React](https://img.shields.io/badge/React-18.3.1-61dafb.svg)](https://reactjs.org)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.6.2-3178c6.svg)](https://www.typescriptlang.org)
[![Ollama](https://img.shields.io/badge/Ollama-Latest-000000.svg)](https://ollama.com)

*Chat with multiple LLMs locally with automatic GPU acceleration. No cloud, no API keys, no tracking. Deploy in seconds with Docker.*

[Features](#-features) ‚Ä¢ [Quick Start](#-quick-start-docker---recommended) ‚Ä¢ [Manual Setup](#-manual-setup-without-docker) ‚Ä¢ [Docker Guide](docs/DOCKER.md) ‚Ä¢ [Development](#-development)

![Monolith Screenshot](https://via.placeholder.com/800x450/1a1a1a/5170ff?text=Monolith+Chat+Interface)

</div>

---

## ‚ú® Features

- üê≥ **One-Command Deployment** - `docker compose up` and you're running with GPU acceleration
- üöÄ **GPU-Accelerated Inference** - Automatic NVIDIA GPU detection and offloading via Ollama
- üì¶ **Pre-installed Model** - Includes `deepseek-r1:8b` automatically downloaded on first run
- üí¨ **Real-time Streaming** - Server-Sent Events for responsive token generation (20-30+ tokens/sec)
- üîÑ **Multiple Backends** - Support for both Ollama (GPU) and llama-cpp-python (CPU)
- üé® **Modern UI** - Clean, responsive interface with markdown rendering and syntax highlighting
- üíæ **Conversation Persistence** - Persistent chat history across container restarts
- üîå **Fully Offline** - No external API calls, all processing happens locally
- üéØ **Model Management** - Easy switching between multiple LLM models
- üåì **Dark Mode Ready** - Professional color scheme optimized for extended use
- ‚ö° **Production Ready** - Health checks, auto-restart, volume management included

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Docker Compose                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ   Frontend   ‚îÇ   ‚îÇ   Backend    ‚îÇ   ‚îÇ   Ollama + GPU       ‚îÇ‚îÇ
‚îÇ  ‚îÇ   (nginx)    ‚îÇ‚óÑ‚îÄ‚ñ∫‚îÇ   (FastAPI)  ‚îÇ‚óÑ‚îÄ‚ñ∫‚îÇ   deepseek-r1:8b     ‚îÇ‚îÇ
‚îÇ  ‚îÇ   Port 3000  ‚îÇ   ‚îÇ   Port 8000  ‚îÇ   ‚îÇ   Port 11434         ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                      ‚îÇ             ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                      ‚ñº             ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ              ‚îÇ NVIDIA GPU   ‚îÇ      ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ              ‚îÇ CUDA Runtime ‚îÇ      ‚îÇ
‚îÇ         ‚îÇ                   ‚ñº              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ         ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ         ‚îÇ           ‚îÇ  Persistent  ‚îÇ                             ‚îÇ
‚îÇ         ‚îÇ           ‚îÇ   Volumes    ‚îÇ                             ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚Ä¢ Chat Data  ‚îÇ                             ‚îÇ
‚îÇ                     ‚îÇ ‚Ä¢ Models     ‚îÇ                             ‚îÇ
‚îÇ                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Quick Start (Docker - Recommended)

### Prerequisites

- **Docker Desktop** - [Download here](https://www.docker.com/products/docker-desktop/)
  - ‚ö†Ô∏è **Important:** Make sure Docker Desktop is running before executing any `docker` commands
  - Look for the Docker whale icon in your system tray (it should say "Docker Desktop is running")
- **NVIDIA GPU** with [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) (for GPU acceleration)
  - Or CPU-only mode (slower but works without GPU)

### One-Command Setup

```bash
# Clone the repository
git clone https://github.com/juliuspleunes4/Monolith.git
cd Monolith

# Start everything (includes automatic deepseek-r1:8b download)
docker compose up

# Open browser to http://localhost:3000
```

**That's it!** The first run will:
1. ‚úÖ Pull pre-built images from Docker Hub (~2-3 minutes)
2. ‚úÖ Start Ollama with GPU support
3. ‚úÖ Automatically download `deepseek-r1:8b` model (~5GB, 5-10 minutes)
4. ‚úÖ Launch the UI at http://localhost:3000

**Total setup time:** 5-10 minutes on first run, then instant afterwards!

> **Pre-built images**: No compilation needed! Images are built automatically and published to Docker Hub.

### GPU Support

**Linux (NVIDIA):**
```bash
# Install NVIDIA Container Toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

**Windows/Mac:**
- Docker Desktop includes GPU support automatically
- Ensure GPU drivers are up to date

### CPU-Only Mode

If you don't have a GPU or NVIDIA Container Toolkit:

```bash
# Use the CPU-only compose file
docker compose -f docker-compose.cpu.yml up
```

‚ö†Ô∏è **Note:** CPU inference is significantly slower (~0.5-2 tokens/sec vs 20-30+ on GPU)

### Docker Commands Reference

```bash
# Start in background
docker compose up -d

# View logs
docker compose logs -f

# Stop all services
docker compose down

# Stop and remove volumes (fresh start)
docker compose down -v

# Add more Ollama models
docker exec -it monolith-ollama ollama pull llama3.2:3b

# Update to latest version
docker compose pull
docker compose up

# Development mode (build from source)
docker compose -f docker-compose.dev.yml up
```

---

## üõ†Ô∏è Manual Setup (Without Docker)

### Prerequisites

- **Python 3.11+**
- **Node.js 18+**
- **Ollama** (for GPU acceleration)
- **NVIDIA GPU** (recommended) or CPU fallback

### 1. Install Ollama

Download and install Ollama from [ollama.com](https://ollama.com):

**Windows:**
```powershell
# Download installer from https://ollama.com/download/windows
# Or use winget:
winget install Ollama.Ollama
```

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Pull an LLM Model

```bash
# Start Ollama service (usually automatic)
ollama serve

# Pull Llama 3.1 8B (recommended, ~5GB)
ollama pull llama3.1:8b

# Or try other models:
ollama pull llama3.2:3b      # Smaller, faster (1.3GB)
ollama pull mistral:7b       # Great general model (4.1GB)
ollama pull qwen2.5:7b       # Excellent for coding (4.7GB)
```

### 3. Clone and Setup

```bash
# Clone the repository
git clone https://github.com/juliuspleunes4/Monolith.git
cd Monolith

# Backend setup
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\Activate.ps1
pip install -r requirements.txt

# Frontend setup
cd ../frontend
npm install

# Start both servers
cd ../backend
python -m uvicorn app.main:app --reload --port 8000

# In another terminal
cd frontend
npm run dev
```

### 4. Open and Chat

Open your browser to **http://localhost:3001** and start chatting! üéâ

## üì¶ Installation

### Backend Setup

The backend is built with FastAPI and supports multiple LLM inference engines.

```bash
cd backend

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows PowerShell:
.\venv\Scripts\Activate.ps1
# Linux/macOS:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Start the backend
python -m uvicorn app.main:app --reload --port 8000
```

**Backend runs on:** `http://localhost:8000`

### Frontend Setup

The frontend is built with React and TypeScript using Vite.

```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

**Frontend runs on:** `http://localhost:3001`

### Optional: Local .GGUF Models

To use local .gguf model files (CPU inference):

1. Create model directories:
```bash
mkdir -p models/{small,medium,large}
```

2. Download models from [Hugging Face](https://huggingface.co/models?library=gguf):
```bash
# Example: Download gemma-3-1b-it
cd models/small
wget https://huggingface.co/user/model/resolve/main/model.gguf
```

3. Restart the backend - models will appear automatically!

## üéØ Usage

### Selecting Models

Click the **Model** selector in the top-right corner to choose from:

- **OLLAMA** category (‚ö° GPU-accelerated)
  - Automatic GPU offloading
  - 20-30+ tokens/second
  - Recommended for best performance

- **SMALL/MEDIUM/LARGE** categories (CPU)
  - Local .gguf files
  - Fallback option
  - No GPU required

### Managing Ollama Models

**With Docker:**
```bash
# List installed models
docker exec -it monolith-ollama ollama list

# Pull new models
docker exec -it monolith-ollama ollama pull <model-name>

# Remove models
docker exec -it monolith-ollama ollama rm <model-name>

# Show model information
docker exec -it monolith-ollama ollama show <model-name>
```

**Without Docker:**
```bash
# List installed models
ollama list

# Pull new models
ollama pull <model-name>

# Remove models
ollama rm <model-name>
```

Popular models to try:
- `deepseek-r1:8b` - Excellent reasoning (pre-installed with Docker)
- `llama3.1:8b` - Latest Llama, great all-rounder
- `llama3.2:3b` - Smaller, faster variant
- `mistral:7b` - Strong general performance
- `qwen2.5:7b` - Top-tier coding model
- `gemma3:7b` - Google's efficient model

### Conversation Management

- **New Chat**: Click the "+" button to start fresh
- **History**: Previous conversations saved in sidebar
- **Persistence**: Conversations stored in browser localStorage

### Code Blocks

Code in responses is automatically syntax-highlighted with:
- Language detection
- Copy button
- Line numbers
- Professional dark theme

## üîß Configuration

### Environment Variables

Create a `.env` file in the `backend` directory:

```env
# Model directories
MODELS_DIR=../models

# Ollama configuration
OLLAMA_BASE_URL=http://localhost:11434

# Logging
LOG_LEVEL=INFO
```

### Model Parameters

Adjust generation parameters in the chat interface:
- **Temperature**: 0.0 (focused) to 2.0 (creative)
- **Max Tokens**: Maximum response length
- **Top P**: Nucleus sampling threshold

## üõ†Ô∏è Development

### Project Structure

```
monolith/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference.py          # llama-cpp-python inference
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama_inference.py   # Ollama integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py               # Chat endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversations.py      # Conversation management
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py             # Model listing/management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py                   # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInterface.tsx     # Main chat UI
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ModelSelector.tsx     # Model picker
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Sidebar.tsx           # Conversation history
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.tsx
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.ts
‚îú‚îÄ‚îÄ models/                            # Local .gguf models (optional)
‚îÇ   ‚îú‚îÄ‚îÄ small/
‚îÇ   ‚îú‚îÄ‚îÄ medium/
‚îÇ   ‚îî‚îÄ‚îÄ large/
‚îî‚îÄ‚îÄ README.md
```

### API Endpoints

**GET** `/api/v1/models`
- List all available models (Ollama + local)

**POST** `/api/v1/chat`
- Stream chat completions
- Body: `{ model, messages, temperature, max_tokens, top_p }`

**GET** `/api/v1/conversations`
- List all conversations

**POST** `/api/v1/conversations`
- Create new conversation

### Adding Custom Backends

Implement the inference interface in `app/llm/`:

```python
async def generate_streaming(
    model_id: str,
    messages: list[dict[str, str]],
    temperature: float = 0.7,
    max_tokens: int = 512,
    top_p: float = 0.9,
) -> AsyncGenerator[str, None]:
    # Your implementation
    yield token
```

## üêõ Troubleshooting

### Docker Issues

**Container won't start:**
```bash
# Check logs
docker compose logs

# Rebuild from scratch
docker compose down -v
docker compose up --build
```

**GPU not detected:**
```bash
# Verify NVIDIA Container Toolkit
docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi

# Check docker daemon.json has nvidia runtime
cat /etc/docker/daemon.json
```

**Model download fails:**
```bash
# Manually pull model
docker exec -it monolith-ollama ollama pull deepseek-r1:8b
```

See [docs/DOCKER.md](docs/DOCKER.md) for comprehensive Docker troubleshooting.

### Ollama Not Detected (Manual Setup)

```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama service
# Windows: Restart from system tray
# macOS/Linux: 
systemctl restart ollama
```

### GPU Not Being Used

```bash
# Verify GPU is available
nvidia-smi  # For NVIDIA
# Ollama automatically uses GPU if available

# Check Ollama logs
ollama logs
```

### Backend Connection Issues

- Ensure backend is running on port 8000
- Check CORS settings in `app/main.py`
- Verify firewall isn't blocking localhost connections

### Slow Performance

- Use **Ollama models** for GPU acceleration
- Reduce `max_tokens` for faster responses
- Try smaller models (llama3.2:3b instead of llama3.1:8b)
- Close other GPU-intensive applications

## üìù License

MIT License - see [LICENSE.md](LICENSE.md) for details

## üìö Documentation

- **[Docker Deployment Guide](docs/DOCKER.md)** - Comprehensive Docker setup and troubleshooting
- **[API Documentation](docs/API.md)** - REST API reference (coming soon)
- **[Changelog](docs/CHANGELOG.md)** - Version history and updates

## üôè Acknowledgments

- [Ollama](https://ollama.com) - Simplified LLM inference with automatic GPU support
- [FastAPI](https://fastapi.tiangolo.com) - Modern Python web framework
- [React](https://reactjs.org) - UI library
- [Docker](https://www.docker.com) - Containerization platform
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - LLM inference engine
- [Hugging Face](https://huggingface.co) - Model repository

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ‚≠ê Show Your Support

Give a ‚≠ê if this project helped you! It helps others discover Monolith.

## üìß Contact

Questions? Open an [issue](https://github.com/juliuspleunes4/Monolith/issues) or start a [discussion](https://github.com/juliuspleunes4/Monolith/discussions)!

---

<div align="center">

**Built with ‚ù§Ô∏è for the local LLM community**

*Run AI locally. Keep your data private. Stay in control.*

[‚¨Ü Back to Top](#-monolith)

</div>
